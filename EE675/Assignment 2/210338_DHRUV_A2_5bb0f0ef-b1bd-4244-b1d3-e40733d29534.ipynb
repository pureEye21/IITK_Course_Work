{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Chooses an item from a list based on associated weights.\n",
        "def choose_weighted_item(items, weights):\n",
        "    total_weight = sum(weights)\n",
        "    random_value = random.uniform(0, total_weight)\n",
        "    cumulative_weight = 0\n",
        "    for item, weight in zip(items, weights):\n",
        "        cumulative_weight += weight\n",
        "        if cumulative_weight >= random_value:\n",
        "            return item\n",
        "    raise AssertionError(\"Shouldn't get here\")\n",
        "\n",
        "# Represents a Markov Decision Process.\n",
        "class MarkovDecisionProcess:\n",
        "    def __init__(self, state_transitions, state_rewards, initial_state=None):\n",
        "        self._validate_parameters(state_transitions, state_rewards)\n",
        "        self._state_transitions = state_transitions\n",
        "        self._state_rewards = state_rewards\n",
        "        self._initial_state = initial_state\n",
        "        self.state_count = len(state_transitions)\n",
        "        self.reset_current_state()\n",
        "\n",
        "    # Returns all states in the MDP.\n",
        "    def get_all_states(self):\n",
        "        return tuple(self._state_transitions.keys())\n",
        "\n",
        "    # Returns valid actions for a given state.\n",
        "    def get_valid_actions(self, state):\n",
        "        return tuple(self._state_transitions.get(state, {}).keys())\n",
        "\n",
        "    # Checks if a state is terminal.\n",
        "    def is_terminal_state(self, state):\n",
        "        return not bool(self.get_valid_actions(state))\n",
        "\n",
        "    # Returns probabilities of next states given a state and action.\n",
        "    def get_next_state_probabilities(self, state, action):\n",
        "        if action not in self.get_valid_actions(state):\n",
        "            raise AssertionError(f\"cannot do action {action} from state {state}\")\n",
        "        return self._state_transitions[state][action]\n",
        "\n",
        "    # Returns the probability of transitioning to a specific next state.\n",
        "    def get_transition_probability(self, state, action, next_state):\n",
        "        return self.get_next_state_probabilities(state, action).get(next_state, 0.0)\n",
        "\n",
        "    # Returns the reward of transitioning to a specific next state.\n",
        "    def get_transition_reward(self, state, action, next_state):\n",
        "        if action not in self.get_valid_actions(state):\n",
        "            raise AssertionError(f\"cannot do action {action} from state {state}\")\n",
        "        return self._state_rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
        "\n",
        "    # Resets the current state to the initial state.\n",
        "    def reset_current_state(self):\n",
        "        if self._initial_state is None:\n",
        "            self._current_state = random.choice((*self._state_transitions.keys(),))\n",
        "        elif self._initial_state in self._state_transitions:\n",
        "            self._current_state = self._initial_state\n",
        "        elif callable(self._initial_state):\n",
        "            self._current_state = self._initial_state()\n",
        "        else:\n",
        "            raise ValueError(f\"initial state {self._initial_state} should be either a state or a function() -> state\")\n",
        "        return self._current_state\n",
        "\n",
        "    # Executes an action and returns the next state, reward, and done flag.\n",
        "    def perform_action(self, action):\n",
        "        next_states, probabilities = zip(*self.get_next_state_probabilities(self._current_state, action).items())\n",
        "        next_state = choose_weighted_item(next_states, weights=probabilities)\n",
        "        reward = self.get_transition_reward(self._current_state, action, next_state)\n",
        "        is_done = self.is_terminal_state(next_state)\n",
        "        self._current_state = next_state\n",
        "        return next_state, reward, is_done, {}\n",
        "\n",
        "    # Prints the current state.\n",
        "    def display_current_state(self):\n",
        "        print(\"Currently at %s\" % self._current_state)\n",
        "\n",
        "    # Validates input parameters for the MDP.\n",
        "    def _validate_parameters(self, state_transitions, state_rewards):\n",
        "        for state in state_transitions:\n",
        "            assert isinstance(state_transitions[state], dict), \"state_transitions for %s should be a dict\" % state\n",
        "            for action in state_transitions[state]:\n",
        "                assert isinstance(state_transitions[state][action], dict), \"state_transitions for %s, %s should be a dict\" % (state, action)\n",
        "                next_state_probabilities = state_transitions[state][action]\n",
        "                assert len(next_state_probabilities) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
        "                sum_probabilities = sum(next_state_probabilities.values())\n",
        "                assert abs(sum_probabilities - 1) <= 1e-10, \"probabilities for state %s action %s sum to %f\" % (state, action, sum_probabilities)\n",
        "        for state in state_rewards:\n",
        "            assert isinstance(state_rewards[state], dict), \"state_rewards for %s should be a dict\" % state\n",
        "            for action in state_rewards[state]:\n",
        "                assert isinstance(state_rewards[state][action], dict), \"state_rewards for %s, %s should be a dict\" % (state, action)\n",
        "        assert None not in state_transitions, \"Do not use None as a state identifier.\"\n",
        "        assert None not in state_rewards, \"Do not use None as an action identifier.\"\n",
        "\n",
        "# Environment Setup\n",
        "\n",
        "# Grid dimensions.\n",
        "num_rows = 3\n",
        "num_columns = 4\n",
        "all_states = list(range(12))  # States 0 to 11.\n",
        "goal_state = 3\n",
        "hole_state = 7\n",
        "wall_state = 5\n",
        "terminal_states = [goal_state, hole_state]\n",
        "non_terminal_states = [s for s in all_states if s not in terminal_states]  # Includes wall state 5.\n",
        "\n",
        "# Actions.\n",
        "available_actions = ['left', 'right', 'up', 'down']\n",
        "\n",
        "# Action to direction mapping.\n",
        "action_directions = {\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1),\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0)\n",
        "}\n",
        "\n",
        "# Stochastic probabilities.\n",
        "intended_probability = 0.8\n",
        "orthogonal_probability = 0.1  # 0.2 split between two orthogonal directions.\n",
        "\n",
        "# Orthogonal actions per action.\n",
        "orthogonal_actions = {\n",
        "    'up': ['left', 'right'],\n",
        "    'down': ['left', 'right'],\n",
        "    'left': ['up', 'down'],\n",
        "    'right': ['up', 'down']\n",
        "}\n",
        "\n",
        "# Helper functions.\n",
        "def get_row_column(state):\n",
        "    return divmod(state, num_columns)\n",
        "\n",
        "def get_state_index(row, column):\n",
        "    if 0 <= row < num_rows and 0 <= column < num_columns:\n",
        "        return row * num_columns + column\n",
        "    return None\n",
        "\n",
        "# Initialize transition probabilities and rewards.\n",
        "state_transitions = {s: {a: {} for a in available_actions} for s in non_terminal_states}\n",
        "state_rewards = {s: {a: {} for a in available_actions} for s in non_terminal_states}\n",
        "\n",
        "for state in non_terminal_states:\n",
        "    row, column = get_row_column(state)\n",
        "    for action in available_actions:\n",
        "        # Intended direction.\n",
        "        row_change, column_change = action_directions[action]\n",
        "        next_row = row + row_change\n",
        "        next_column = column + column_change\n",
        "        next_state = get_state_index(next_row, next_column)\n",
        "        if next_state is None or next_state == wall_state:\n",
        "            next_state = state\n",
        "        state_transitions[state][action][next_state] = state_transitions[state][action].get(next_state, 0) + intended_probability\n",
        "        state_rewards[state][action][next_state] = 1.0 if next_state == goal_state else -1.0 if next_state == hole_state else -0.01\n",
        "\n",
        "        # Orthogonal directions.\n",
        "        for orthogonal_action in orthogonal_actions[action]:\n",
        "            row_change_orthogonal, column_change_orthogonal = action_directions[orthogonal_action]\n",
        "            next_row_orthogonal = row + row_change_orthogonal\n",
        "            next_column_orthogonal = column + column_change_orthogonal\n",
        "            next_state_orthogonal = get_state_index(next_row_orthogonal, next_column_orthogonal)\n",
        "            if next_state_orthogonal is None or next_state_orthogonal == wall_state:\n",
        "                next_state_orthogonal = state\n",
        "            state_transitions[state][action][next_state_orthogonal] = state_transitions[state][action].get(next_state_orthogonal, 0) + orthogonal_probability\n",
        "            state_rewards[state][action][next_state_orthogonal] = 1.0 if next_state_orthogonal == goal_state else -1.0 if next_state_orthogonal == hole_state else -0.01\n",
        "\n",
        "# Create MDP environment.\n",
        "environment = MarkovDecisionProcess(state_transitions, state_rewards, initial_state=0)\n",
        "\n",
        "# 1. Policy Iteration.\n",
        "\n",
        "# Calculates the value function for a given policy.\n",
        "def calculate_value_function(policy, environment, discount_factor=1.0, convergence_tolerance=1e-8):\n",
        "    value_function = {state: 0.0 for state in all_states}\n",
        "    evaluation_count = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in non_terminal_states:\n",
        "            old_value = value_function[state]\n",
        "            action = policy[state]\n",
        "            state_value = sum(\n",
        "                probability * (environment.get_transition_reward(state, action, next_state) + discount_factor * value_function[next_state])\n",
        "                for next_state, probability in environment.get_next_state_probabilities(state, action).items()\n",
        "            )\n",
        "            value_function[state] = state_value\n",
        "            delta = max(delta, abs(old_value - state_value))\n",
        "            evaluation_count += 1\n",
        "        if delta < convergence_tolerance:\n",
        "            break\n",
        "    return value_function, evaluation_count\n",
        "\n",
        "# Generates an improved policy based on the value function.\n",
        "def generate_improved_policy(environment, value_function, discount_factor=1.0):\n",
        "    policy = {}\n",
        "    for state in non_terminal_states:\n",
        "        action_values = {\n",
        "            action: sum(\n",
        "                probability * (environment.get_transition_reward(state, action, next_state) + discount_factor * value_function[next_state])\n",
        "                for next_state, probability in environment.get_next_state_probabilities(state, action).items()\n",
        "            )\n",
        "            for action in available_actions\n",
        "        }\n",
        "        policy[state] = max(action_values, key=action_values.get)\n",
        "    return policy\n",
        "\n",
        "# Runs the policy iteration algorithm.\n",
        "def run_policy_iteration(environment, discount_factor=1.0, max_iterations=1000):\n",
        "    policy = {state: random.choice(available_actions) for state in non_terminal_states}\n",
        "    total_evaluations = 0\n",
        "    for iteration in range(max_iterations):\n",
        "        value_function, evaluation_count = calculate_value_function(policy, environment, discount_factor)\n",
        "        total_evaluations += evaluation_count\n",
        "        new_policy = generate_improved_policy(environment, value_function, discount_factor)\n",
        "        if new_policy == policy:\n",
        "            print(f\"Policy Iteration converged after {iteration + 1} policy improvements, {total_evaluations} total evaluations\")\n",
        "            return new_policy, value_function\n",
        "        policy = new_policy\n",
        "    print(\"Policy Iteration did not converge within max iterations\")\n",
        "    return policy, value_function\n",
        "\n",
        "# Run Policy Iteration.\n",
        "print('=== Policy Iteration ===')\n",
        "pi_policy, pi_value_function = run_policy_iteration(environment)\n",
        "\n",
        "# Display policy.\n",
        "print('Optimal Policy (PI):')\n",
        "action_symbols = {'up': '^', 'down': 'v', 'left': '<', 'right': '>'}\n",
        "grid = [[' ' for _ in range(num_columns)] for _ in range(num_rows)]\n",
        "for state in all_states:\n",
        "    row, column = get_row_column(state)\n",
        "    if state == goal_state:\n",
        "        grid[row][column] = 'G'\n",
        "    elif state == hole_state:\n",
        "        grid[row][column] = 'H'\n",
        "    elif state == wall_state:\n",
        "        grid[row][column] = 'W'\n",
        "    elif state in pi_policy:\n",
        "        grid[row][column] = action_symbols[pi_policy[state]]\n",
        "for row in grid:\n",
        "    print(' '.join(row))\n",
        "\n",
        "# 2. Value Iteration.\n",
        "\n",
        "# Runs the value iteration algorithm.\n",
        "def run_value_iteration(environment, discount_factor=1.0, convergence_tolerance=1e-8, max_iterations=1000):\n",
        "    value_function = {state: 0.0 for state in all_states}\n",
        "    for iteration in range(max_iterations):\n",
        "        delta = 0\n",
        "        for state in non_terminal_states:\n",
        "            old_value = value_function[state]\n",
        "            action_values = [sum(probability * (environment.get_transition_reward(state, action, next_state) + discount_factor * value_function[next_state])\n",
        "                                    for next_state, probability in environment.get_next_state_probabilities(state, action).items())\n",
        "                                    for action in available_actions]\n",
        "            value_function[state] = max(action_values)\n",
        "            delta = max(delta, abs(old_value - value_function[state]))\n",
        "        if delta < convergence_tolerance:\n",
        "            print(f'Value Iteration converged after {iteration+1} iterations')\n",
        "            break\n",
        "    policy = generate_improved_policy(environment, value_function, discount_factor)\n",
        "    return policy, value_function\n",
        "\n",
        "# Run Value Iteration.\n",
        "print('\\n=== Value Iteration ===')\n",
        "vi_policy, vi_value_function = run_value_iteration(environment)\n",
        "\n",
        "# Display policy.\n",
        "print('Optimal Policy (VI):')\n",
        "grid = [[' ' for _ in range(num_columns)] for _ in range(num_rows)]\n",
        "for state in all_states:\n",
        "    row, column = get_row_column(state)\n",
        "    if state == goal_state:\n",
        "        grid[row][column] = 'G'\n",
        "    elif state == hole_state:\n",
        "        grid[row][column] = 'H'\n",
        "    elif state == wall_state:\n",
        "        grid[row][column] = 'W'\n",
        "    elif state in vi_policy:\n",
        "        grid[row][column] = action_symbols[vi_policy[state]]\n",
        "for row in grid:\n",
        "    print(' '.join(row))\n",
        "\n",
        "# 3. Comparison of PI and VI.\n",
        "\n",
        "print('\\n=== Comparison ===')\n",
        "policies_equal = pi_policy == vi_policy\n",
        "print(f'Policies are {\"identical\" if policies_equal else \"different\"}')\n",
        "print('Convergence comparison is based on the output above:')\n",
        "print('- PI reports number of policy improvements and total evaluations (iterations within policy evaluation).')\n",
        "print('- VI reports total iterations until value function convergence.')\n",
        "print('Typically, VI converges faster in terms of total computational steps, as PI requires multiple evaluations per policy update.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUHuVTZKIstJ",
        "outputId": "59792737-e074-473d-e0c0-37eb3b41ef9c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Policy Iteration ===\n",
            "Policy Iteration converged after 5 policy improvements, 317400 total evaluations\n",
            "Optimal Policy (PI):\n",
            "> > > G\n",
            "^ W < H\n",
            "^ < < v\n",
            "\n",
            "=== Value Iteration ===\n",
            "Value Iteration converged after 148 iterations\n",
            "Optimal Policy (VI):\n",
            "> > > G\n",
            "^ W < H\n",
            "^ < < v\n",
            "\n",
            "=== Comparison ===\n",
            "Policies are identical\n",
            "Convergence comparison is based on the output above:\n",
            "- PI reports number of policy improvements and total evaluations (iterations within policy evaluation).\n",
            "- VI reports total iterations until value function convergence.\n",
            "Typically, VI converges faster in terms of total computational steps, as PI requires multiple evaluations per policy update.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q7N0LPvTJL4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}